# ğŸš€ Proyecto ETL con Airflow, GCP, Terraform, DBT y Secret Manager

## ğŸ§  DescripciÃ³n general

Este proyecto implementa una **ETL moderna en Google Cloud Platform (GCP)** que extrae datos de una **API pÃºblica (Rick & Morty)**, los transforma y los carga en **BigQuery** siguiendo el **modelo Medallion (bronze â†’ silver â†’ gold)**.  

El flujo combina diversas herramientas de ingenierÃ­a de datos:  

- **Airflow** (orquestaciÃ³n)  
- **Cloud Function** (procesamiento en GCS)  
- **BigQuery** (almacenamiento estructurado)  
- **Terraform** (infraestructura como cÃ³digo)  
- **DBT** (transformaciones SQL)  
- **Secret Manager** (manejo seguro de credenciales)  
- **Docker + WSL** (entorno de desarrollo local)

---

## ğŸ—ï¸ Arquitectura general

```
                +------------------+
                |    Rick & Morty  |
                |       API        |
                +--------+---------+
                         |
                         v
                 +-------+--------+
                 |   GCS Bucket   |
                 |     raw/       |
                 +-------+--------+
                         |
                         | (disparo manual o programado)
                         v
                 +-------+--------+
                 |  Cloud Function|
                 |  (Python / BQ) |
                 +-------+--------+
                         |
                +--------+--------+
                |   GCS Bucket    |
                |   processed/    |
                +--------+--------+
                         |
                         v
              +----------+-----------+
              |     BigQuery          |
              | bronze â†’ silver â†’ gold|
              +----------+-----------+
                         |
                         v
                +--------+--------+
                |   GCS Bucket    |
                |   historic/     |
                +-----------------+

```

ğŸ“ AdemÃ¡s, existe una carpeta `schema/` en el bucket donde se almacena el **esquema JSON** de la tabla bronze de BigQuery, lo que permite mantener control de versiones sobre la estructura de los datos.

---

## ğŸ§© Componentes principales

### ğŸŒ€ 1. Airflow (Docker + WSL)
- Configurado mediante `docker-compose.yaml`.  
- Orquesta los DAGs que coordinan el flujo de extracciÃ³n, carga y verificaciÃ³n. 
- Usa un operador personalizado que consume la api y de inmediato almacena en GCS. 
- El DAG principal es `dag_rick_and_morty_with_operator.py`, que:
  1. Llama a la API pÃºblica y almacena los datos en un archivo JSON.   
  2. EnvÃ­a el archivo JSON al bucket `raw/`.  
  3. Llama a la Cloud Function encargada de procesarlo y generar el archivo Parquet.  
  4. Inserta los datos en BigQuery (bronze).  
  5. Mueve los datos a la carpeta `historic/`.  

### â˜ï¸ 2. Cloud Function
- Desplegada con **Terraform** (automatizaciÃ³n total).  
- Procesa los archivos del bucket `raw/`:  
  - Convierte JSON â†’ Parquet.  
  - Escribe en `processed/`.  
  - Inserta datos en BigQuery.  
- Usa **Service Account** con permisos mÃ­nimos: lectura de GCS, escritura en BigQuery.

### ğŸ—ƒï¸ 3. BigQuery (modelo Medallion)
Organiza los datasets en tres capas:

| Capa | DescripciÃ³n |
|------|--------------|
| **bronze** | Datos crudos directamente del Parquet procesado por la Cloud Function. |
| **silver** | Limpieza, tipificaciÃ³n y enriquecimiento con DBT. |
| **gold** | Agregaciones o vistas finales para anÃ¡lisis y dashboards. |

### ğŸ§± 4. Terraform
- Despliega la **Cloud Function**, las **Service Accounts** y los permisos IAM necesarios.  
- Facilita la recreaciÃ³n del entorno sin pasos manuales.

### ğŸ§® 5. DBT
- Se usa para aplicar transformaciones SQL dentro de BigQuery.  
- Configurado con `profiles.yml` y modelo Medallion.  
- Ejecutado manualmente o mediante Airflow.

### ğŸ”’ 6. Secret Manager
- Gestiona variables sensibles (claves, IDs de proyecto, tokens API).  
- Evita incluir credenciales en el cÃ³digo fuente o archivos `.env`.

### ğŸ³ 7. WSL + Docker
- Entorno local reproducible para ejecutar Airflow y DBT.  
- Permite simular el flujo completo antes de desplegar en GCP.

---

## ğŸ“‚ Estructura del repositorio

```
data_engineering_project/
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ docker-compose.yaml
â”‚   â”œâ”€â”€ config/airflow.cfg
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ dag_rick_and_morty_with_operator.py
â”‚   â”‚   â”œâ”€â”€ dag_prueba.py
â”‚   â”‚   â””â”€â”€ __pycache__/
â”‚   â””â”€â”€ logs/
â”‚
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ cloud_functions_source/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ modules/
â”‚       â”œâ”€â”€ cloud_function/
â”‚       â”‚   â”œâ”€â”€ main.py
â”‚       |   |â”€â”€ variables.tf
â”‚       |   â””â”€â”€ outputs.tf
â”‚       â”œâ”€â”€ main.tf
â”‚       â”œâ”€â”€ variables.tf
â”‚       â””â”€â”€ terraform.tfvars
â”‚
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ dbt_data_engineering_project/
â”‚   |  â”œâ”€â”€ macros/
â”‚   |  â”œâ”€â”€ models/
|   |      â”œâ”€â”€bronze
|   |      â””â”€â”€silver
â”‚   |      â”œâ”€â”€gold
â”‚   â””â”€â”€ dbt_project.yml
â”‚
â””â”€â”€ README.md   â† este archivo
```

---

## âš™ï¸ ConfiguraciÃ³n inicial

1. **Clonar el repositorio**  
   ```bash
   git clone https://github.com/tu_usuario/data_engineering_project.git
   cd data_engineering_project
   ```

2. **Configurar .yml airflow**  
Airflow se configura directamente en el archivo `docker-compose.yml`.  
AllÃ­ se definen las variables necesarias para la conexiÃ³n con GCP y el uso de **Secret Manager**.  

Ejemplo de variables clave:

```yaml
environment:
  AIRFLOW__API_AUTH__JWT_SECRET: "supersecretjwt"
  GOOGLE_APPLICATION_CREDENTIALS: "/opt/keys/gcp_service_account.json"
  AIRFLOW__SECRETS__BACKEND: "airflow.providers.google.cloud.secrets.secret_manager.CloudSecretManagerBackend"
  AIRFLOW__SECRETS__BACKEND_KWARGS: '{"connections_prefix": "airflow-connections", "variables_prefix": "airflow-variables", "project_id": "data-engineering-dev-464423"}'
```

AdemÃ¡s, se monta la clave del **Service Account** de GCP con el siguiente volumen:
```yaml
volumes:
  - ${AIRFLOW_PROJ_DIR:-.}/../keys/gcp_service_account.json:/opt/keys/gcp_service_account.json
```

Esto permite que Airflow acceda a los secretos y credenciales almacenados en GCP sin necesidad de exponerlas en texto plano ni usar archivos `.env`.

---

3. **Levantar Airflow en WSL/Docker**  
   ```bash
   cd airflow
   docker-compose up -d
   ```

4. **Desplegar Cloud Function con Terraform**
   ```bash
   cd cloud_function/terraform
   terraform init
   terraform apply
   ```

5. **Configurar DBT**
   ```bash
   cd dbt
   dbt run
   ```

---

## â–¶ï¸ EjecuciÃ³n del flujo ETL

1. Airflow ejecuta el DAG `dag_rick_and_morty_with_operator.py`.  
2. Se obtiene el JSON desde la API y se guarda en `raw/`.  
3. La Cloud Function procesa el archivo y genera un Parquet en `processed/`.  
4. Los datos se cargan en BigQuery (tabla bronze).  
5. DBT transforma los datos hasta la capa gold.  
6. El archivo original pasa a `historic/`.  

---

## ğŸ” Monitoreo y mantenimiento

- **Airflow UI:** Verifica la ejecuciÃ³n y logs de tareas.  
- **GCP Console:** Monitorea la Cloud Function y BigQuery.  
- **DBT logs:** Para transformaciones SQL.  
- **Secret Manager:** Revisa versiones de credenciales.

---

## ğŸ’¡ Buenas prÃ¡cticas aplicadas

- Infraestructura como cÃ³digo (Terraform).  
- SeparaciÃ³n de capas en BigQuery (modelo Medallion).  
- Seguridad con Secret Manager.  
- Formato **Parquet** para eficiencia y bajo costo.  
- Procesamiento **event-driven** (basado en archivos).  
- Modularidad: cada componente puede evolucionar de forma independiente.

---

## âœ¨ Autor

**Juan Guillermo Quintero**  
Ingeniero de Datos | Proyecto ETL con GCP, Airflow, DBT, Terraform y Secret Manager  
